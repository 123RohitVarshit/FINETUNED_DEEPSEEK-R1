{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETGZno8fE00n"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "WUBkJxh_Mezj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "from huggingface_hub import login\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import wandb\n",
        "from google.colab import userdata\n",
        "userdata.get('Hugging_Face_Token')\n",
        "userdata.get('wnb')"
      ],
      "metadata": {
        "id": "79CEKpX_Jm_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_token=userdata.get('Hugging_Face_Token')\n",
        "wnb_token=userdata.get('wnb')\n",
        "\n",
        "login(HF_token)\n",
        "wandb.login(key=wnb_token)\n",
        "run=wandb.init(\n",
        "    project=\"Fine tune DeepSeek-R1-Llama-8B on Medical COT Dataset\",\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ],
      "metadata": {
        "id": "bGbnGVZoMdrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,tokenizer=FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    token=HF_token\n",
        ")"
      ],
      "metadata": {
        "id": "UF2STwXtm5s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_style=\"\"\"Below is an instruction that describes a task ,paired with an input that provides further context .\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering,think carefully about the question and create a step by step chain-of-thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced medical knowledge in clinical reasoning,diagonstics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CDOP-ROisATh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating test medical question for inference\n",
        "question=\"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing and sneezing but no leakage at night undergoes a gynelogical exam\n",
        "         and Q-tip test.Based on this findings, what would csytometry most likely reveal about her residual volume and detrusor contractions? \"\"\"\n",
        "\n",
        "# Enabling optimized infernece mode for unsloth models\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Format the question using structured prompt (prompt_style) and tokenize it.\n",
        "inputs=tokenizer([prompt_style.format(question,\"\")],return_tensors='pt').to('cuda')\n",
        "\n",
        "# Generate response using the model\n",
        "outputs=model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "# Decode response into human readable text\n",
        "response=tokenizer.batch_decode(outputs)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L0F_bHK8u0t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and print only relevant response part(after ### Response)\n",
        "print(response[0].split('### Response')[1])"
      ],
      "metadata": {
        "id": "bO8MZiCN9EJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_prompt_style=\"\"\"Below is an instruction that describes a task ,paired with an input that provides further context .\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering,think carefully about the question and create a step by step chain-of-thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced medical knowledge in clinical reasoning,diagonstics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "mKsNDHXO9QDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download medical dataset from HuggingFace using load_dataset function\n",
        "dataset=load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\",split=\"train[0:500]\",trust_remote_code=True)\n",
        "dataset"
      ],
      "metadata": {
        "id": "bkRXLg9u__Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1]"
      ],
      "metadata": {
        "id": "OsA3tg1dBkLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to format dataset according to our prompt training style\n",
        "EOS_TOKEN=tokenizer.eos_token\n",
        "EOS_TOKEN"
      ],
      "metadata": {
        "id": "wEUZJczPBomy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining formatting prompt function\n",
        "def formatting_prompt_func(examples):\n",
        "  inputs=examples['Question']\n",
        "  cots=examples['Complex_CoT']\n",
        "  outputs=examples['Response']\n",
        "\n",
        "  texts=[]\n",
        "\n",
        "  for input,cot,output in zip(inputs,cots,outputs):\n",
        "   text=train_prompt_style.format(input,cot,output) + EOS_TOKEN\n",
        "   texts.append(text)\n",
        "\n",
        "  return{\"text\":texts}\n"
      ],
      "metadata": {
        "id": "gHIeNZR_CZl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update dataset formatting\n",
        "dataset_finetune=dataset.map(formatting_prompt_func,batched=True)\n",
        "dataset_finetune[\"text\"][0]"
      ],
      "metadata": {
        "id": "lAkUyIbdE_K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying LoRA(Low Rank Adaption)fine tuning to the model\n",
        "model_lora=FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None\n",
        ")"
      ],
      "metadata": {
        "id": "EeKZRT4JGlEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model_lora,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset_finetune,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    dataset_num_proc=2,\n",
        "\n",
        "    # Training arguments\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "FmYeMfXcTnhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start fine tuning process and saving the fine tuned model\n",
        "trainer_stats=trainer.train()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "BppeiJ2sXo5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating test medical question for inference\n",
        "question=\"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing and sneezing but no leakage at night undergoes a gynelogical exam\n",
        "         and Q-tip test.Based on this findings, what would csytometry most likely reveal about her residual volume and detrusor contractions? \"\"\"\n",
        "\n",
        "# Enabling optimized infernece mode for unsloth models\n",
        "FastLanguageModel.for_inference(model_lora)\n",
        "\n",
        "# Format the question using structured prompt (prompt_style) and tokenize it.\n",
        "inputs=tokenizer([prompt_style.format(question,\"\")],return_tensors='pt').to('cuda')\n",
        "\n",
        "# Generate response using the model\n",
        "outputs=model_lora.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "# Decode response into human readable text\n",
        "response=tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Extract and print only relevant response part(after ### Response)\n",
        "print(response[0].split('### Response:')[1])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xuiSzxbJfKOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}